{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c55f228",
   "metadata": {},
   "source": [
    "# Prepare data in the format for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614fa147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "import sys\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import geopy.distance\n",
    "\n",
    "def add_sin_and_cos_features(df, column_to_transform):\n",
    "    df['Sin_{}'.format(column_to_transform)] = np.sin(2 * np.pi * df[column_to_transform] / max(df[column_to_transform])) \n",
    "    df['Cos_{}'.format(column_to_transform)] = np.cos(2 * np.pi * df[column_to_transform] / max(df[column_to_transform]))\n",
    "    return df\n",
    "\n",
    "sys.path.append('../../3. Modelling')\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006ffe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size_m = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef86b98",
   "metadata": {},
   "source": [
    "### Prepare footfall data\n",
    "<u> Including removing outliers:</u>  \n",
    "The model should predict normal footfall. Therefore any days that have extremely high or low footfall should be taken out of the training data. We don't actually want the model to try to predict footfall on unusual days, because the things that make the day unusual (like errors in the camera counters, or the presence of special events) are not captured in the input data.\n",
    "\n",
    "Outliers are detected using the Median Absolute Deviation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574774e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "sensors = pd.read_csv('../../Cleaned_data/SensorData/allsensors.csv')\n",
    "# Create month as number not string\n",
    "sensors['datetime'] =pd.to_datetime(sensors['datetime'], format = '%Y-%m-%d %H:%M:%S')#dayfirst = False)\n",
    "# Keep only data from 2011 onwards\n",
    "sensors= sensors[sensors['year']>2010]\n",
    "# # Create a categrorial variable defining the time of day\n",
    "sensors['time_of_day'] = sensors.apply (lambda row: label_hours(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "sensors, outliers = remove_outliers(sensors)\n",
    "# Drop unneeded columns\n",
    "sensors=sensors.drop(['Latitude', 'Longitude', 'location', 'mdate'], axis=1)\n",
    "# Check the data\n",
    "sensors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72ace2",
   "metadata": {},
   "source": [
    "### Inspect outliers which have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "acdc79d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>outlier</th>\n",
       "      <th>hourly_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2651422</th>\n",
       "      <td>2019-11-15 23:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "      <td>15979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2792250</th>\n",
       "      <td>2020-02-28 22:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "      <td>14437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175360</th>\n",
       "      <td>2018-10-26 18:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "      <td>12289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544340</th>\n",
       "      <td>2019-08-23 22:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>11742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694353</th>\n",
       "      <td>2019-12-18 17:00:00</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "      <td>11612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime  sensor_id  outlier  hourly_counts\n",
       "2651422 2019-11-15 23:00:00         57     True          15979\n",
       "2792250 2020-02-28 22:00:00         57     True          14437\n",
       "2175360 2018-10-26 18:00:00         57     True          12289\n",
       "2544340 2019-08-23 22:00:00          7     True          11742\n",
       "2694353 2019-12-18 17:00:00         57     True          11612"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at outliers from particular day (which have reason to believe might have incldued outliers)\n",
    "t = [pd.to_datetime('2020-03-07 13:00:00')]\n",
    "this_day = outliers[outliers['datetime'].isin(t)]#[2255323]['datetime']\n",
    "# Look at biggest outliers\n",
    "test = outliers.nlargest(5, 'hourly_counts')\n",
    "# outliers[outliers['sensor_id']==57]\n",
    "# fig,ax=plt.subplots(figsize=(4,10))\n",
    "# outliers['sensor_id'].value_counts().sort_values().plot(kind = 'barh',ax=ax)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09504d7b",
   "metadata": {},
   "source": [
    "### Inspecting sensor which has a lot of the outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor_57 = sensors[sensors['sensor_id']==57]\n",
    "# # Filter by a single date\n",
    "# sensor_57_oneday = sensor_57[sensor_57['datetime'].dt.strftime('%Y-%m-%d') == \"2019-11-15\"]\n",
    "# sensor_57_oneday.index=sensor_57_oneday['time']\n",
    "# fig, ax = plt.subplots(figsize = (5,4), sharex = True)\n",
    "# fig = sensor_57_oneday['hourly_counts'].plot(ax=ax, color='darkred', linewidth=2, legend =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf563693",
   "metadata": {},
   "source": [
    "## add date variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622560dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors['day_of_month_num'] = sensors['datetime'].dt.day\n",
    "sensors['weekday_num'] = sensors['datetime'].dt.weekday +1\n",
    "sensors['month_num'] = sensors['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9efa4",
   "metadata": {},
   "source": [
    "### Join public holiday and weather data to sensor data (WHY DO NUMBER OF ROWS INCREASE SLIGHTLY?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08bfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features = sensors.merge(pd.read_csv('../../Cleaned_data/WeatherData/weather_data_allyears.csv', parse_dates=['datetime']), on='datetime', how='left')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/WeatherData/DailyRainfallData.csv', parse_dates=['datetime']), on='datetime', how='left')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/HolidaysData/publicholidays.csv', parse_dates=['datetime']),how='left', on='datetime')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/HolidaysData/schoolholidays.csv', parse_dates=['datetime']),how='left', on='datetime')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/SpatialFeatures/sensors_betweenness.csv'),how='left', on='sensor_id')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/FeaturesNearSensors/num_features_near_sensors_{}.csv'.format(buffer_size_m), index_col=0) ,how='left', on='sensor_id')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/FeaturesNearSensors/feature_subtypes_near_sensors_{}.csv'.format(buffer_size_m), index_col=0) ,how='left', on='sensor_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90f9d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length is now:  4129082\n"
     ]
    }
   ],
   "source": [
    "print(\"Length is now: \" , len(sensors_with_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4ea08",
   "metadata": {},
   "source": [
    "### Add average number of floors of building in vicinity, for correct year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features['avg_n_floors'] = sensors_with_features.apply (lambda row: select_n_floors(row), axis=1)\n",
    "sensors_with_features = sensors_with_features[sensors_with_features.columns.drop(list(sensors_with_features.filter(regex='avg_n_floors_')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06562c2",
   "metadata": {},
   "source": [
    "### Add buildings (correctly for the year the data relates to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09781314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SENSORS WITH FEATURES\n",
    "sensors_with_features['buildings'] = sensors_with_features.apply (lambda row: select_buildings(row), axis=1).copy()\n",
    "sensors_with_features= sensors_with_features.drop(['buildings_2010', 'buildings_2011','buildings_2012', 'buildings_2013',\n",
    "                                                  'buildings_2014','buildings_2015','buildings_2016','buildings_2017',\n",
    "                                                  'buildings_2018', 'buildings_2019', 'buildings_2020'], axis =1)\n",
    "# FOR SENSORS WITH SUBFEATURES\n",
    "# Create a dataframe containing just the building subttypes for the year that this row refers to\n",
    "temp = pd.DataFrame(None)\n",
    "# For each year, get the data for just that year\n",
    "for year in range(2011,2022+1):   \n",
    "    # Get just footfall data for this year\n",
    "    this_year = sensors_with_features[sensors_with_features['year'] == year]\n",
    "    # If year is over 2020 then set the year to 2020 for the purposes of selecting the building data \n",
    "    if year > 2020:\n",
    "        year = 2020\n",
    "    # Get just the building columns for this year\n",
    "    buildings_this_yr = this_year.filter(like='{}'.format(year))\n",
    "    # Drop all the building subtype columns from the row (and the bikes) \n",
    "    this_year = this_year[this_year.columns.drop(list(this_year.filter(regex='bikes|buildings_')))]\n",
    "     # Join the row without any buildings, back to this row \n",
    "    this_year = pd.concat([this_year, buildings_this_yr], axis=1)\n",
    "    # Rename -- ??\n",
    "    this_year.columns = this_year.columns.str.replace(r'_{}'.format(year), '')\n",
    "    # Join to dataframe which will store data for all years eventually\n",
    "    temp = temp.append(this_year)\n",
    "sensors_with_features = temp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55504128",
   "metadata": {},
   "source": [
    "### Add dummy variables for calendar variables \n",
    "(Not doing this anymore as creating cos and sin variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensors_with_features=convert_df_variables_to_dummy(sensors_with_features, ['day', 'month', 'year', 'time'])\n",
    "# sensors_with_features=convert_df_variables_to_dummy(sensors_with_features, ['time_of_day'])\n",
    "# sensors_with_features = sensors_with_features.drop(['datetime'],axis=1)\n",
    "\n",
    "### Date based variables: Option 2 - Create Dummy Variables\n",
    "for date_col in ['day', 'month',]:\n",
    "    date_col_dummy =  pd.get_dummies(sensors_with_features[date_col], drop_first = True)\n",
    "    if date_col =='month':\n",
    "        date_col_dummy.columns= prepend(date_col_dummy.columns.values, 'month_')\n",
    "    # if date_col =='year':\n",
    "        # date_col_dummy.columns= prepend(date_col_dummy.columns.values, 'year_')\n",
    "    sensors_with_features = pd.concat([sensors_with_features, date_col_dummy],axis=1)\n",
    "    del sensors_with_features[date_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc43ef",
   "metadata": {},
   "source": [
    "### Create sin/cos variables to represent cyclical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a55eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time refers to the hour of the day 0-23 (makes sense that this is cyclical - relationship between 23 and 0)\n",
    "sensors_with_features = add_sin_and_cos_features(sensors_with_features, 'time')\n",
    "# Month number from 1-12 (makes sense that this is cyclical - relationship between 12 and 1)\n",
    "sensors_with_features = add_sin_and_cos_features(sensors_with_features, 'month_num')\n",
    "# Weekday number from 1-7 (maybe doesnt make sense that this is cyclical - doesn't really follow a logical pattern?\n",
    "sensors_with_features = add_sin_and_cos_features(sensors_with_features, 'weekday_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431335c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sensors_with_features.plot.scatter('Sin_weekday_num', 'Cos_weekday_num').set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b536828",
   "metadata": {},
   "source": [
    "### Replace NaNs with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features= sensors_with_features.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704467ef",
   "metadata": {},
   "source": [
    "## Create aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Filter to include just sensors which we know have quite complete data \n",
    "# data = sensors_with_features[sensors_with_features['sensor_id'].isin([2,6,9,10,14,18])]\n",
    "# data.reset_index(inplace=True, drop = True)\n",
    "\n",
    "# # Get just this hourly counts\n",
    "# hourly_counts = data[['datetime', \"hourly_counts\"]]\n",
    "# # Get the sum of all values for one hour\n",
    "# summed_hourly_counts = hourly_counts.groupby(\"datetime\").sum()\n",
    "# # reset index (for joining)\n",
    "# summed_hourly_counts.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# # Get just the features\n",
    "# features = data[['datetime','Temp', 'Humidity', 'Pressure', 'Rain', 'WindSpeed',\n",
    "#        'public_holiday','school_holiday','Sin_time', 'Cos_time', 'Sin_month_num', 'Cos_month_num', 'Sin_weekday_num', 'Cos_weekday_num']]\n",
    "# # Keep only one version of each row (should all be the same)\n",
    "# features = features.drop_duplicates(keep='last')\n",
    "# # reset index (for joining)\n",
    "# features.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# # Join features to aggregated count values\n",
    "# aggregated = pd.concat([summed_hourly_counts, features], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a279d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in sensors_with_features.columns:\n",
    "#     print(column, np.nanmax(sensors_with_features[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0536d9",
   "metadata": {},
   "source": [
    "### Remove time variables no longer required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e941fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensors_with_features = sensors_with_features.drop([ 'Sin_month_num', 'Cos_month_num', 'Sin_weekday_num',\n",
    "#        'Cos_weekday_num',],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fea2ac",
   "metadata": {},
   "source": [
    "### Add distance from centre as a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in Melbourne sensor location spatial data\n",
    "melbourne_sensors = pd.read_csv(\"../../Data/FootfallData/melbourne_locations.csv\")\n",
    "melbourne_sensors.rename(columns={'sensor_description': 'Name'}, inplace = True)\n",
    "melbourne_sensors = melbourne_sensors.drop_duplicates('sensor_id', keep='first')\n",
    "\n",
    "# Coordinates of 'centre' of CBD (done on google maps)\n",
    "coords_1 = (-37.812187461761596, 144.962265054567)\n",
    "distances =[]\n",
    "for row_number in range(0,len(melbourne_sensors)):\n",
    "    coords_2 = (melbourne_sensors['Latitude'][row_number], melbourne_sensors['Longitude'][row_number])\n",
    "    distances.append(geopy.distance.geodesic(coords_1, coords_2).km)\n",
    "    \n",
    "melbourne_sensors['distance_from_centre']=distances\n",
    "melbourne_sensors=melbourne_sensors[['sensor_id','distance_from_centre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features= pd.merge(sensors_with_features, melbourne_sensors, on=['sensor_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ad911",
   "metadata": {},
   "source": [
    "### Save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca71733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features.to_csv(\"../../Cleaned_data/FormattedDataForModelling/formatted_data_for_modelling_allsensors_{}.csv\".format(buffer_size_m),\n",
    "                            index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
